# -*- coding: utf-8 -*-
"""top10-recommendation-linkedin-job-posting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FLyMguvg7Tkv5Q5kY-mgvIMDpUCbn-11
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install sentence-transformers

from google.colab import files
files.upload()  # This will prompt you to upload `kaggle.json`
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install kaggle
!mkdir -p /content/datasets

# Download LinkedIn job postings dataset
!kaggle datasets download -d arshkon/linkedin-job-postings -p /content/datasets
!unzip -q /content/datasets/linkedin-job-postings.zip -d /content/datasets/linkedin_job_postings

print('Data source import complete.')

!pip install nltk
import nltk
nltk.download('wordnet')

nltk.download('punkt')
nltk.download('stopwords')
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity

# Download necessary NLTK data files


# import warnings
# warnings.filterwarnings('ignore')

df = pd.read_csv('/content/datasets/linkedin_job_postings/postings.csv')

df.head()

df.shape

df.isnull().sum()

# Select relevant columns and drop duplicates
df = df[['title', 'description']].drop_duplicates().dropna()

"""# Preprocessing"""

# Preprocess text data
nltk.download('punkt_tab')
def preprocess_text(text):
    # lowecasing
    text = text.lower()
    # remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # tokenize
    words = word_tokenize(text)
    # remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    # lemmatization the words
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join the words back into a full string
    preprocess_text = ' '.join(words)

    return preprocess_text

df['description'] = df['description'].apply(preprocess_text)

"""# Split Into Train and Test Sets"""

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

"""### Random Sampling"""

sample_train_df = train_df.sample(frac=0.005, random_state=42)

sample_train_df.shape

# Define the pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('clf', LogisticRegression(solver='liblinear', class_weight='balanced'))
])

# Parameter grid
param_grid = {
    'tfidf__max_df': [0.75, 0.85, 1.0],
    'tfidf__min_df': [1, 2, 5],
    'tfidf__ngram_range': [(1, 1), (1, 2)],
    'clf__C': [0.1, 1, 10]
}

# Perform RandomizedSearchCV
grid_search = RandomizedSearchCV(pipeline, param_grid, cv=2, verbose=2, n_jobs=-1, n_iter=10)
grid_search.fit(sample_train_df['description'], sample_train_df['title'])

# Update vectorizer with best parameters
best_params = grid_search.best_params_
vectorizer = TfidfVectorizer(stop_words='english',
                             max_df=best_params['tfidf__max_df'],
                             min_df=best_params['tfidf__min_df'],
                             ngram_range=best_params['tfidf__ngram_range'])

# Fit the vectorizer on the full dataset
tfidf_matrix = vectorizer.fit_transform(sample_train_df['description'])

from sentence_transformers import SentenceTransformer, util

# Reset the index of the DataFrame
sample_train_df = sample_train_df.reset_index(drop=True)

# load pre-trained model
model = SentenceTransformer('all-mpnet-base-v2')

# encode job description
job_embeddings = model.encode(sample_train_df['description'].tolist(), convert_to_tensor=True)

def search_jobs(query, job_embeddings, df, model, vectorizer, top_n=10):
    # Preprocess and encode the query with BERT
    query = preprocess_text(query)
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Compute cosine similarity with BERT embeddings
    cosine_scores_bert = util.pytorch_cos_sim(query_embedding, job_embeddings)[0]

    # Transform query with TF-IDF vectorizer and compute cosine similarity
    query_tfidf = vectorizer.transform([query])
    cosine_scores_tfidf = cosine_similarity(query_tfidf, tfidf_matrix).flatten()

    # Combine scores (e.g., weighted sum)
    combined_scores = 0.5 * cosine_scores_bert.cpu().numpy() + 0.5 * cosine_scores_tfidf

    # Get top N most similar jobs
    top_indices = np.argsort(combined_scores)[-top_n:][::-1]
    top_jobs = df.iloc[top_indices]

    return top_jobs

# Make predictions on the test set
predicted_labels = grid_search.predict(test_df['description'])
true_labels = t
  est_df['title']

# Define evaluation function
def evaluate_model(true_labels, predicted_labels):
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels, average='weighted')
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    print(f"Accuracy: {accuracy}\nPrecision: {precision}\nRecall: {recall}\nF1-Score: {f1}")

# Evaluate the model
evaluate_model(true_labels, predicted_labels)

query = 'i love software engineer, i have bachelor degree on computer science'
top_jobs = search_jobs(query, job_embeddings, df, model, vectorizer)
top_jobs